{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings = pd.read_csv('data/movies/ratings.csv')\n",
    "df_movies = pd.read_csv('data/movies/movies.csv')\n",
    "df_links = pd.read_csv('data/movies/links.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "ft_model = KeyedVectors.load_word2vec_format('models/wiki-news-300d-1M-subword.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('desks', 0.8139737844467163), ('desk-', 0.8030417561531067), ('desk.', 0.778192400932312), ('front-desk', 0.7296270132064819), ('ref-desk', 0.7272905111312866), ('deskside', 0.7197455167770386), ('help-desk', 0.715452253818512), ('writing-desk', 0.7056628465652466), ('refdesk', 0.6872211694717407), ('Desk', 0.6861226558685303)]\n"
     ]
    }
   ],
   "source": [
    "print (ft_model.most_similar('desk'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def clean_text(text, remove_stopwords=True):\n",
    "    '''Clean the text, with the option to remove stopwords'''\n",
    "    text = text.lower()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"russian\"))\n",
    "        tokens = [w for w in tokens if not w in stops]\n",
    "    \n",
    "#     text = \" \".join(tokens)\n",
    "    return tokens\n",
    "\n",
    "def is_ascii(s):\n",
    "    return all(ord(c) < 128 for c in s)\n",
    "\n",
    "df_movies = df_movies.drop(df_movies[df_movies['title'].apply(lambda t: not is_ascii(t))].index)\n",
    "\n",
    "def process_title(title): \n",
    "    # strip away numbers and parenthesis\n",
    "    title = title.replace('(','').replace(')','')\n",
    "    title = re.sub(r'\\d+','',title)\n",
    "    # strip away \"part\" word\n",
    "    title = re.sub(r'[Pp]art','',title)\n",
    "    #strip II and III and IV\n",
    "    title = title.replace('II','').replace('III','').replace('IV','')\n",
    "    return title\n",
    "\n",
    "df_movies['title'] = df_movies['title'].apply(process_title) \n",
    "#drop empty titles\n",
    "df_movies = df_movies.drop(df_movies[df_movies['title'].str.strip() ==''].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_dim = 300\n",
    "\n",
    "def create_average_vec(doc):\n",
    "    average = np.zeros((vec_dim,), dtype='float32')\n",
    "    num_words = 0.\n",
    "    for word in doc:\n",
    "        if word in ft_model.wv.vocab:\n",
    "            average = np.add(average, ft_model[word])\n",
    "            num_words += 1.\n",
    "    if num_words != 0.:\n",
    "        average = np.divide(average, num_words)\n",
    "    return average\n",
    "\n",
    "\n",
    "def create_doc2vec(text):\n",
    "    processed_text = clean_text(text)\n",
    "    vec = create_average_vec(processed_text)\n",
    "    return vec\n",
    "\n",
    "data['doc2vec'] = urls['title'].apply(create_doc2vec)\n",
    "urls.to_csv('data/movies/movies_titles_with_fasttext.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create links to imdb website\n",
    "def make_urls(df_links):\n",
    "    df_links['pagePath'] = 'https://www.imdb.com/title/tt00' + df_links['imdbId'].astype(str)\n",
    "    df_links.drop(['imdbId'],axis=1,inplace=True)\n",
    "    df_links.drop(['tmdbId'],axis=1,inplace=True)\n",
    "    return df_links\n",
    "df_links = make_urls(df_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge by userId all csv files\n",
    "def merge_df(df_ratings, df_links, df_movies):\n",
    "    df_result = pd.merge(df_ratings, df_links, on='movieId', how='left')\n",
    "    df_result = pd.merge(df_result, df_movies, on='movieId', how='left')\n",
    "    df_result['visitStartTime'] = df_result['timestamp']\n",
    "    df_result.set_index(['userId', 'movieId'], inplace=True)\n",
    "    df_result = df_result[['visitStartTime', 'pagePath', 'title', 'genres', 'rating']]\n",
    "    return df_result\n",
    "df_result = merge_df(df_ratings, df_links, df_movies)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

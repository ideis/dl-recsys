{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk, re\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_month = pd.read_csv('data/news/texts_31days.csv', index_col=0)\n",
    "texts_first = pd.read_csv('data/news/texts_12.04-13.04.csv', index_col=0)\n",
    "texts_second = pd.read_csv('data/news/texts_13.04-14.04.csv', index_col=0)\n",
    "texts = pd.concat([texts_month, texts_first, texts_second])\n",
    "texts.index.names = ['url_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using texts.csv to make urls for each url_id\n",
    "def make_urls_df():\n",
    "    texts = pd.read_csv('data/news/texts.csv')\n",
    "\n",
    "    tag_cleaned = texts['tag'].str.split().str.get(0)\n",
    "    texts['tag_cleaned'] = tag_cleaned\n",
    "    texts['url_id'] = texts['url_id'].astype(str)\n",
    "    texts['pagePath'] = '/t/' + texts['tag_cleaned'] + '/' + texts['url_id']\n",
    "\n",
    "    urls = texts.drop(['subtitle', 'tag', 'tag_cleaned'], axis=1)\n",
    "    return urls\n",
    "\n",
    "urls = make_urls_df()\n",
    "urls.dropna(how='any', inplace=True)\n",
    "urls.drop_duplicates(['title'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaning\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def clean_text(text, remove_stopwords=True):\n",
    "    '''Clean the text, with the option to remove stopwords'''\n",
    "    text = text.lower()\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"russian\"))\n",
    "        tokens = [w for w in tokens if not w in stops]\n",
    "    \n",
    "#     text = \" \".join(tokens)\n",
    "    return tokens\n",
    "\n",
    "# PoS tagging\n",
    "from ufal.udpipe import Model, Pipeline\n",
    "modelfile = 'models/udpipe_syntagrus.model'\n",
    "\n",
    "def tag_ud(text, modelfile='udpipe_syntagrus.model'):\n",
    "    model = Model.load(modelfile)\n",
    "    pipeline = Pipeline(model, 'tokenize', Pipeline.DEFAULT, Pipeline.DEFAULT, 'conllu')\n",
    "    processed = pipeline.process(text)\n",
    "    output = [l for l in processed.split('\\n') if not l.startswith('#')]\n",
    "    tagged = [w.split('\\t')[2].lower() + '_' + w.split('\\t')[3] for w in output if w]\n",
    "    tagged_propn = []\n",
    "    propn  = []\n",
    "    for t in tagged:\n",
    "        if t.endswith('PROPN'):\n",
    "            if propn:\n",
    "                propn.append(t)\n",
    "            else:\n",
    "                propn = [t]\n",
    "        else:\n",
    "            if len(propn) > 1:\n",
    "                name = '::'.join([x.split('_')[0] for x in propn]) + '_PROPN'\n",
    "                tagged_propn.append(name)\n",
    "            elif len(propn) == 1:\n",
    "                tagged_propn.append(propn[0])\n",
    "            tagged_propn.append(t)\n",
    "            propn = []\n",
    "    return tagged_propn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "ft_model = gensim.models.fasttext.FastText.load('models/araneum_none_fasttextcbow_300_5_2018/araneum_none_fasttextcbow_300_5_2018.model')\n",
    "# word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(\"models/word2vec/ruscorpora_upos_skipgram_300_5_2018.vec\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc2vec for every news title\n",
    "vec_dim = 300\n",
    "\n",
    "def create_average_vec(doc):\n",
    "    average = np.zeros((vec_dim,), dtype='float32')\n",
    "    num_words = 0.\n",
    "    for word in doc:\n",
    "        if word in ft_model.wv.vocab:\n",
    "            average = np.add(average, ft_model[word])\n",
    "            num_words += 1.\n",
    "    if num_words != 0.:\n",
    "        average = np.divide(average, num_words)\n",
    "    return average\n",
    "\n",
    "\n",
    "def create_doc2vec(text):\n",
    "    text = str(text)\n",
    "    processed_text = clean_text(text)\n",
    "#     processed_ud = tag_ud(text=processed_text, modelfile=modelfile)\n",
    "    vec = create_average_vec(processed_text)\n",
    "    return vec\n",
    "\n",
    "urls['doc2vec'] = urls['title'].apply(create_doc2vec)\n",
    "urls.to_csv('data/news/urls_with_unique_titles_fasttext.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USEFUL CODE STARTS HERE\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "from datetime import timezone\n",
    "from zipfile import ZipFile\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = pd.read_csv('data/news/urls_with_unique_titles_fasttext.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_df(start_time, end_time):\n",
    "    timestamps = sorted(os.listdir('data/news/timestamps'))\n",
    "    start_timestamp, end_timestamp = make_timestamps_from_datetime(start_time, end_time, timestamps)\n",
    "    interval = make_interval(start_timestamp, end_timestamp, timestamps)\n",
    "    with ZipFile('data/news/timestamps.zip') as timestamps_zip:\n",
    "        df_list = [pd.read_csv(timestamps_zip.open(\"timestamps/\" + file), header=None, names=['fullVisitorId', 'url_id', 'visitStartTime']) for file in interval]\n",
    "    df = pd.concat(df_list)\n",
    "    labels, levels = pd.factorize(df['fullVisitorId'])\n",
    "    df['user_id'] = labels\n",
    "    return df\n",
    "\n",
    "\n",
    "# first = 12/03/2017 07:00:00(1491818423), last = 14/04/2017(1491991225) 11:11:29\n",
    "def make_timestamps_from_datetime(start_time, end_time, timestamps):\n",
    "    if start_time == 'first':\n",
    "        start_timestamp = timestamps[0]\n",
    "    else:\n",
    "        start_datetime = datetime.strptime(start_time, '%d/%m/%Y %H:%M:%S')\n",
    "        start_timestamp = (start_datetime - datetime(1970, 1, 1)).total_seconds()\n",
    "        \n",
    "    if end_time == 'last':\n",
    "        end_timestamp = timestamps[-1]\n",
    "    else:\n",
    "        end_datetime = datetime.strptime(end_time, '%d/%m/%Y %H:%M:%S')\n",
    "        end_timestamp = (end_datetime - datetime(1970, 1, 1)).total_seconds()\n",
    "    return (start_timestamp, end_timestamp)\n",
    "\n",
    "\n",
    "def make_interval(start_timestamp, end_timestamp, timestamps):\n",
    "    start_timestamp = str(start_timestamp)\n",
    "    end_timestamp = str(end_timestamp)\n",
    "    interval = [t for t in timestamps if t >= start_timestamp and t <= end_timestamp]\n",
    "    return interval\n",
    "\n",
    "\n",
    "def merge_df(df, urls):\n",
    "    urls['url_id'] = urls['url_id'].astype(int)\n",
    "    urls['title'] = urls['title'].astype(str)\n",
    "    \n",
    "    df_result = pd.merge(df, urls, on='url_id', how='left')\n",
    "\n",
    "    labels, levels = pd.factorize(df_result['url_id'])\n",
    "    df_result['url_id'] = labels\n",
    "    df_result.set_index(['user_id', 'url_id'], inplace=True)\n",
    "    df_result.sort_index(inplace=True)\n",
    "    df_result.dropna(how='any',inplace=True)\n",
    "    df_result.drop_duplicates(inplace=True)\n",
    "    \n",
    "    df_result['fullVisitorId'] = df_result['fullVisitorId'].astype(str)\n",
    "    df_result['title'] = df_result['title'].astype(str)\n",
    "    df_result['pagePath'] = df_result['pagePath'].astype(str)\n",
    "\n",
    "    return df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can choose time interval beetween 12/03/2017 07:00:00 and 14/04/2017 11:11:29\n",
    "# Also you can use as arguments 'first' or 'last'\n",
    "start_time = '12/04/2017 10:00:00'\n",
    "stop_time = '14/04/2017 11:11:29'\n",
    "\n",
    "df = make_df(start_time, stop_time)\n",
    "df_result = merge_df(df, urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculates user clicks score for each news article\n",
    "scores = df_result['pagePath'].value_counts()\n",
    "df_result['scores'] = df_result['pagePath'].apply(lambda path: scores[str(path)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONALLY\n",
    "df_result.to_csv('data/news/news_final_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_result.info())\n",
    "display(df_result.head(20))\n",
    "display(df_result.tail(20))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
